"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—Å–µ—Ö sitemap —Ñ–∞–π–ª–æ–≤
–ó–∞–ø—É—Å–∫: python generate_sitemaps.py
"""
import os
import asyncio
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import select, func
from datetime import datetime
from dotenv import load_dotenv

from models.review import Review

load_dotenv()

SUPPORTED_LANGS = ["ru", "en", "uk", "ro"]
DEFAULT_LANG = "ru"
SITEMAP_DIR = "static/sitemaps"
os.makedirs(SITEMAP_DIR, exist_ok=True)

BASE_URL = os.getenv("BASE_URL", "https://safelogist.net").rstrip('/')


def normalize_lang(lang: str) -> str:
    lang_code = (lang or "").lower()
    return lang_code if lang_code in SUPPORTED_LANGS else DEFAULT_LANG


def save_sitemap(content: str, filename: str) -> None:
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å sitemap –≤ —Ñ–∞–π–ª"""
    filepath = os.path.join(SITEMAP_DIR, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"‚úì –°–æ—Ö—Ä–∞–Ω—ë–Ω: {filename}")


async def generate_sitemap_pages(db: AsyncSession, lang: str, page_num: int, total_companies: int):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è sitemap –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ —Å–ø–∏—Å–∫–∞ –æ—Ç–∑—ã–≤–æ–≤"""
    companies_per_page = 10
    total_pages = max(1, (total_companies + companies_per_page - 1) // companies_per_page)
    
    # –ú–∞–∫—Å–∏–º—É–º 40,000 URL –Ω–∞ —Ñ–∞–π–ª (–≥–ª–∞–≤–Ω–∞—è + –ø–∞–≥–∏–Ω–∞—Ü–∏—è)
    max_urls_per_sitemap = 40000
    pages_per_sitemap = max_urls_per_sitemap - 1  # -1 –¥–ª—è –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    
    start_page = (page_num - 1) * pages_per_sitemap + 1
    end_page = min(start_page + pages_per_sitemap - 1, total_pages)
    
    urls = []
    
    # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Ç–æ–ª—å–∫–æ –≤ –ø–µ—Ä–≤–æ–º sitemap
    if page_num == 1:
        urls.append(f"""  <url>
    <loc>{BASE_URL}/{lang}/reviews</loc>
    <lastmod>{datetime.now().date().isoformat()}</lastmod>
    <changefreq>daily</changefreq>
    <priority>1.0</priority>
  </url>""")
        start_page = 2
    
    # –°—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    for page in range(start_page, end_page + 1):
        urls.append(f"""  <url>
    <loc>{BASE_URL}/{lang}/reviews?page={page}</loc>
    <lastmod>{datetime.now().date().isoformat()}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.9</priority>
  </url>""")
    
    if not urls:
        return False
    
    sitemap = f"""<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd">
{chr(10).join(urls)}
</urlset>
"""

    filename = f"sitemap-pages-{lang}-{page_num}.xml"
    save_sitemap(sitemap, filename)
    return True


async def generate_sitemap_companies(db: AsyncSession, lang: str, page: int, last_subject: str = None):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è sitemap –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–π —Å –ª–∏–º–∏—Ç–æ–º 45,000 URL, –Ω–æ –≤–∫–ª—é—á–∞—è –í–°–ï —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞–∂–¥–æ–π –∫–æ–º–ø–∞–Ω–∏–∏"""
    lang_code = normalize_lang(lang)
    max_urls_per_sitemap = 45000  # –ñ–µ—Å—Ç–∫–∏–π –ª–∏–º–∏—Ç
    reviews_per_page = 10
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ (–±–µ—Ä–µ–º —Å –∑–∞–ø–∞—Å–æ–º, —á—Ç–æ–±—ã —Ç–æ—á–Ω–æ —Ö–≤–∞—Ç–∏–ª–æ)
    query = (
        select(
            Review.subject.label("subject"),
            func.min(Review.id).label("company_id"),
            func.count(Review.id).label("reviews_count")
        )
        .group_by(Review.subject)
        .order_by(Review.subject)
    )
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å last_subject, –Ω–∞—á–∏–Ω–∞–µ–º —Å –Ω–µ–≥–æ (–¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –ø–æ—Å–ª–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ñ–∞–π–ª–∞)
    if last_subject:
        query = query.where(Review.subject > last_subject)
    
    query = query.limit(10000)  # –ë–µ—Ä–µ–º —Å –±–æ–ª—å—à–∏–º –∑–∞–ø–∞—Å–æ–º
    result = await db.execute(query)
    companies = result.all()
    
    if not companies:
        return None, False  # –ù–µ—Ç –∫–æ–º–ø–∞–Ω–∏–π –¥–ª—è —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    
    urls = []
    current_url_count = 0
    last_processed_subject = None
    
    for row in companies:
        company_id = row.company_id
        subject = row.subject
        reviews_count = row.reviews_count or 0
        total_pages = max(1, (reviews_count + reviews_per_page - 1) // reviews_per_page)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –¥–ª—è —ç—Ç–æ–π –∫–æ–º–ø–∞–Ω–∏–∏
        company_urls_count = total_pages  # 1 –≥–ª–∞–≤–Ω–∞—è + (total_pages - 1) –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–º –ª–∏ –ª–∏–º–∏—Ç, –¥–æ–±–∞–≤–∏–≤ –í–°–ï —Å—Ç—Ä–∞–Ω–∏—Ü—ã —ç—Ç–æ–π –∫–æ–º–ø–∞–Ω–∏–∏
        if current_url_count + company_urls_count > max_urls_per_sitemap and current_url_count > 0:
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è, –Ω–µ –¥–æ–±–∞–≤–ª—è—è —ç—Ç—É –∫–æ–º–ø–∞–Ω–∏—é
            break
        
        # –î–æ–±–∞–≤–ª—è–µ–º –í–°–ï —Å—Ç—Ä–∞–Ω–∏—Ü—ã —ç—Ç–æ–π –∫–æ–º–ø–∞–Ω–∏–∏
        # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–æ—Å–Ω–æ–≤–Ω–∞—è)
        urls.append(f"""  <url>
    <loc>{BASE_URL}/{lang_code}/reviews/item/{company_id}</loc>
    <lastmod>{datetime.now().date().isoformat()}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>""")

        # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        if total_pages > 1:
            for page_num in range(2, total_pages + 1):
                urls.append(f"""  <url>
    <loc>{BASE_URL}/{lang_code}/reviews/item/{company_id}?page={page_num}</loc>
    <lastmod>{datetime.now().date().isoformat()}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.7</priority>
  </url>""")
        
        current_url_count += company_urls_count
        last_processed_subject = subject
    
    if not urls:
        return None, False
    
    sitemap = f"""<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd">
{chr(10).join(urls)}
</urlset>
"""

    filename = f"sitemap-{lang_code}-{page}.xml"
    save_sitemap(sitemap, filename)
    print(f"   ‚Üí {filename}: {current_url_count} URLs")
    return last_processed_subject, True


async def generate_sitemap_index():
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–ª–∞–≤–Ω–æ–≥–æ sitemap index –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤"""
    sitemaps = []
    
    # –ò—â–µ–º –≤—Å–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ sitemap —Ñ–∞–π–ª—ã
    for filename in sorted(os.listdir(SITEMAP_DIR)):
        if filename.startswith("sitemap-") and filename.endswith(".xml"):
            sitemaps.append(f"""  <sitemap>
    <loc>{BASE_URL}/{filename}</loc>
    <lastmod>{datetime.now().date().isoformat()}</lastmod>
  </sitemap>""")
    
    sitemap_index = f"""<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/siteindex.xsd">
{chr(10).join(sitemaps)}
</sitemapindex>
"""

    save_sitemap(sitemap_index, "sitemap.xml")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    database_url = os.getenv("DATABASE_URL")
    if not database_url:
        print("‚ùå –û—à–∏–±–∫–∞: DATABASE_URL –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ .env")
        return

    print(f"üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö...")
    engine = create_async_engine(database_url)
    async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

    async with async_session() as db:
        print(f"üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è sitemap –¥–ª—è {BASE_URL}\n")

        # 1. –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–∞–Ω–∏–π
        count_query = select(func.count(func.distinct(Review.subject)))
        count_result = await db.execute(count_query)
        total_companies = count_result.scalar() or 0
        
        print(f"\n1. –ù–∞–π–¥–µ–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {total_companies}\n")
        
        # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º sitemap –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ —Å–ø–∏—Å–∫–∞ –æ—Ç–∑—ã–≤–æ–≤
        print("2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è sitemap –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞–≥–∏–Ω–∞—Ü–∏–∏...")
        companies_per_page = 10
        total_pages = max(1, (total_companies + companies_per_page - 1) // companies_per_page)
        max_urls_per_sitemap = 40000
        pages_per_sitemap = max_urls_per_sitemap - 1
        num_pages_sitemaps = max(1, (total_pages + pages_per_sitemap - 1) // pages_per_sitemap)
        
        for lang in SUPPORTED_LANGS:
            for page_num in range(1, num_pages_sitemaps + 1):
                await generate_sitemap_pages(db, lang, page_num, total_companies)

        # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º sitemap –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–π
        print("\n3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è sitemap –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–π...")
        generated_files = []
        
        for lang in SUPPORTED_LANGS:
            page = 1
            last_subject = None
            
            while True:
                last_subject, success = await generate_sitemap_companies(db, lang, page, last_subject)
                if not success:
                    break  # –ù–µ—Ç –±–æ–ª—å—à–µ –∫–æ–º–ø–∞–Ω–∏–π
                generated_files.append(f"sitemap-{lang}-{page}.xml")
                page += 1
        
        print(f"\n   –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(generated_files)}")

        # 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≥–ª–∞–≤–Ω—ã–π sitemap index
        print("\n4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è sitemap.xml (index)...")
        await generate_sitemap_index()

        print(f"\n‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {SITEMAP_DIR}/")

    await engine.dispose()


if __name__ == "__main__":
    asyncio.run(main())

